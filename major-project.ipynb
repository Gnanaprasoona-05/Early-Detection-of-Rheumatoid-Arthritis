{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2097406,"sourceType":"datasetVersion","datasetId":1257880}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\ntrain_path = '/kaggle/input/knee-osteoarthritis-dataset-with-severity/train'\ntest_path = '/kaggle/input/knee-osteoarthritis-dataset-with-severity/test'\nfiltered_train_path = '/kaggle/working/filtered_train'\nfiltered_test_path = '/kaggle/working/filtered_test'\n\nclasses_to_keep = ['0', '3', '4']\n\nos.makedirs(filtered_train_path, exist_ok=True)\nos.makedirs(filtered_test_path, exist_ok=True)\n\ndef copy_images(src_dir, dst_dir, class_name,num_class):\n    os.makedirs(os.path.join(dst_dir, class_name))\n    src_class_dir = os.path.join(src_dir, class_name)\n    dst_class_dir = os.path.join(dst_dir, class_name)\n    images = os.listdir(src_class_dir)\n    random.shuffle(images)\n    for img in images[:num_class]:\n        shutil.copy(os.path.join(src_class_dir, img), os.path.join(dst_class_dir, img))\n\nfor class_name in classes_to_keep:\n    copy_images(train_path, filtered_train_path, class_name,2286)\n\nfor class_name in classes_to_keep:\n    copy_images(test_path, filtered_test_path, class_name,639)\n\nfor dir_path in [filtered_train_path, filtered_test_path]:\n    print(f\"Directory: {dir_path}\")\n    for class_name in classes_to_keep:\n        print(f\"{class_name}: {len(os.listdir(os.path.join(dir_path, class_name)))} images\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-16T14:49:23.040038Z","iopub.execute_input":"2024-03-16T14:49:23.040426Z","iopub.status.idle":"2024-03-16T14:49:52.613724Z","shell.execute_reply.started":"2024-03-16T14:49:23.040389Z","shell.execute_reply":"2024-03-16T14:49:52.612806Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Directory: /kaggle/working/filtered_train\n0: 2286 images\n3: 757 images\n4: 173 images\nDirectory: /kaggle/working/filtered_test\n0: 639 images\n3: 223 images\n4: 51 images\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading Datasets","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = ImageFolder(root=\"/kaggle/working/filtered_train\", transform=transform)\ntest_dataset = ImageFolder(root=\"/kaggle/working/filtered_test\", transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T14:49:52.615307Z","iopub.execute_input":"2024-03-16T14:49:52.615606Z","iopub.status.idle":"2024-03-16T14:50:03.271736Z","shell.execute_reply.started":"2024-03-16T14:49:52.615581Z","shell.execute_reply":"2024-03-16T14:50:03.270952Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import time\n\ndef train(model, train_loader, criterion, optimizer, num_epochs=5):\n    for epoch in range(num_epochs):\n        start_time = time.time()  # Record start time of epoch\n        model.train()\n        running_loss = 0.0\n        correct_predictions = 0\n        total_samples = 0\n        \n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            batch_size = inputs.size(0)  # Get the batch size\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * batch_size\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_samples += batch_size\n        \n        epoch_loss = running_loss / total_samples\n        epoch_acc = correct_predictions / total_samples\n        end_time = time.time()  # Record end time of epoch\n        epoch_time = end_time - start_time  # Calculate time taken for epoch\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}, Time: {epoch_time:.2f} seconds\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T14:50:03.272844Z","iopub.execute_input":"2024-03-16T14:50:03.273330Z","iopub.status.idle":"2024-03-16T14:50:03.282137Z","shell.execute_reply.started":"2024-03-16T14:50:03.273301Z","shell.execute_reply":"2024-03-16T14:50:03.281373Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Performance metrics","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\ndef evaluate(model, test_loader):\n    model.eval()\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n\n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions, average='weighted')\n    recall = recall_score(all_labels, all_predictions, average='weighted')\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n\n    return accuracy, precision, recall, f1\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T14:50:03.284356Z","iopub.execute_input":"2024-03-16T14:50:03.284627Z","iopub.status.idle":"2024-03-16T14:50:03.952274Z","shell.execute_reply.started":"2024-03-16T14:50:03.284604Z","shell.execute_reply":"2024-03-16T14:50:03.951154Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Mobilenet","metadata":{}},{"cell_type":"code","source":"from torchvision.models import mobilenet_v2\nmodel_mobilenet = mobilenet_v2(pretrained=True)\nnum_ftrs = model_mobilenet.classifier[1].in_features\nmodel_mobilenet.classifier[1] = nn.Linear(num_ftrs, 3)  # Assuming 3 classes\n\nmodel_mobilenet = model_mobilenet.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_mobilenet.parameters(), lr=0.001)\ntrain(model_mobilenet, train_loader, criterion, optimizer, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T14:50:03.953534Z","iopub.execute_input":"2024-03-16T14:50:03.953949Z","iopub.status.idle":"2024-03-16T14:55:36.240896Z","shell.execute_reply.started":"2024-03-16T14:50:03.953923Z","shell.execute_reply":"2024-03-16T14:55:36.239911Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 98.3MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 0.3364, Accuracy: 0.8682, Time: 18.85 seconds\nEpoch 2/20, Loss: 0.1953, Accuracy: 0.9260, Time: 16.10 seconds\nEpoch 3/20, Loss: 0.1240, Accuracy: 0.9562, Time: 16.12 seconds\nEpoch 4/20, Loss: 0.0996, Accuracy: 0.9658, Time: 16.22 seconds\nEpoch 5/20, Loss: 0.0736, Accuracy: 0.9770, Time: 16.19 seconds\nEpoch 6/20, Loss: 0.0499, Accuracy: 0.9820, Time: 16.26 seconds\nEpoch 7/20, Loss: 0.0762, Accuracy: 0.9764, Time: 16.32 seconds\nEpoch 8/20, Loss: 0.0319, Accuracy: 0.9894, Time: 16.38 seconds\nEpoch 9/20, Loss: 0.0675, Accuracy: 0.9779, Time: 16.42 seconds\nEpoch 10/20, Loss: 0.0539, Accuracy: 0.9810, Time: 16.46 seconds\nEpoch 11/20, Loss: 0.0644, Accuracy: 0.9764, Time: 16.54 seconds\nEpoch 12/20, Loss: 0.0516, Accuracy: 0.9832, Time: 16.61 seconds\nEpoch 13/20, Loss: 0.0268, Accuracy: 0.9928, Time: 16.73 seconds\nEpoch 14/20, Loss: 0.1566, Accuracy: 0.9475, Time: 16.62 seconds\nEpoch 15/20, Loss: 0.0275, Accuracy: 0.9925, Time: 16.73 seconds\nEpoch 16/20, Loss: 0.0283, Accuracy: 0.9904, Time: 16.58 seconds\nEpoch 17/20, Loss: 0.0121, Accuracy: 0.9960, Time: 16.75 seconds\nEpoch 18/20, Loss: 0.0059, Accuracy: 0.9981, Time: 16.56 seconds\nEpoch 19/20, Loss: 0.0314, Accuracy: 0.9885, Time: 16.63 seconds\nEpoch 20/20, Loss: 0.0558, Accuracy: 0.9826, Time: 16.54 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy, precision, recall, f1 = evaluate(model_mobilenet, test_loader)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T14:55:36.242352Z","iopub.execute_input":"2024-03-16T14:55:36.242639Z","iopub.status.idle":"2024-03-16T14:55:38.711710Z","shell.execute_reply.started":"2024-03-16T14:55:36.242615Z","shell.execute_reply":"2024-03-16T14:55:38.710835Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Accuracy: 0.9441\nPrecision: 0.9462\nRecall: 0.9441\nF1-score: 0.9449\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Densenet","metadata":{}},{"cell_type":"code","source":"from torchvision.models import densenet121\n\n# Load pre-trained DenseNet-121 model\nmodel_densenet = densenet121(pretrained=True)\n\n# Modify the classifier to match the number of classes in your dataset\nnum_ftrs = model_densenet.classifier.in_features\nmodel_densenet.classifier = nn.Linear(num_ftrs, 3)  # Assuming 3 classes\n\n# Move the model to the device (GPU if available)\nmodel_densenet = model_densenet.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_densenet.parameters(), lr=0.001)\n\n# Train the model\ntrain(model_densenet, train_loader, criterion, optimizer, num_epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T14:55:38.712958Z","iopub.execute_input":"2024-03-16T14:55:38.713306Z","iopub.status.idle":"2024-03-16T15:01:47.240734Z","shell.execute_reply.started":"2024-03-16T14:55:38.713274Z","shell.execute_reply":"2024-03-16T15:01:47.239811Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:00<00:00, 124MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.3838, Accuracy: 0.8495, Time: 37.09 seconds\nEpoch 2/10, Loss: 0.2121, Accuracy: 0.9251, Time: 36.76 seconds\nEpoch 3/10, Loss: 0.1626, Accuracy: 0.9437, Time: 36.75 seconds\nEpoch 4/10, Loss: 0.1632, Accuracy: 0.9400, Time: 36.71 seconds\nEpoch 5/10, Loss: 0.1164, Accuracy: 0.9602, Time: 36.74 seconds\nEpoch 6/10, Loss: 0.1167, Accuracy: 0.9580, Time: 36.72 seconds\nEpoch 7/10, Loss: 0.0980, Accuracy: 0.9633, Time: 36.82 seconds\nEpoch 8/10, Loss: 0.0853, Accuracy: 0.9695, Time: 36.70 seconds\nEpoch 9/10, Loss: 0.0934, Accuracy: 0.9667, Time: 36.77 seconds\nEpoch 10/10, Loss: 0.0540, Accuracy: 0.9792, Time: 36.83 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy, precision, recall, f1 = evaluate(model_densenet, test_loader)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T15:01:47.242139Z","iopub.execute_input":"2024-03-16T15:01:47.242420Z","iopub.status.idle":"2024-03-16T15:01:51.559375Z","shell.execute_reply.started":"2024-03-16T15:01:47.242396Z","shell.execute_reply":"2024-03-16T15:01:51.558390Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Accuracy: 0.9255\nPrecision: 0.9312\nRecall: 0.9255\nF1-score: 0.9273\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Resnet","metadata":{}},{"cell_type":"code","source":"from torchvision.models import resnet18\n\n# Load pre-trained ResNet-18 model\nmodel_resnet18 = resnet18(pretrained=True)\n\n# Modify the classifier to match the number of classes in your dataset\nnum_ftrs = model_resnet18.fc.in_features\nmodel_resnet18.fc = nn.Linear(num_ftrs, 3)  # Assuming 3 classes\n\n# Move the model to the device (GPU if available)\nmodel_resnet18 = model_resnet18.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_resnet18.parameters(), lr=0.001)\n\n# Train the model\ntrain(model_resnet18, train_loader, criterion, optimizer, num_epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T15:01:59.106511Z","iopub.execute_input":"2024-03-16T15:01:59.107215Z","iopub.status.idle":"2024-03-16T15:04:24.031912Z","shell.execute_reply.started":"2024-03-16T15:01:59.107186Z","shell.execute_reply":"2024-03-16T15:04:24.030744Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 150MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.4554, Accuracy: 0.8218, Time: 14.66 seconds\nEpoch 2/10, Loss: 0.2451, Accuracy: 0.9111, Time: 14.55 seconds\nEpoch 3/10, Loss: 0.1728, Accuracy: 0.9369, Time: 14.40 seconds\nEpoch 4/10, Loss: 0.1278, Accuracy: 0.9546, Time: 14.49 seconds\nEpoch 5/10, Loss: 0.0998, Accuracy: 0.9639, Time: 14.31 seconds\nEpoch 6/10, Loss: 0.0974, Accuracy: 0.9639, Time: 14.32 seconds\nEpoch 7/10, Loss: 0.0822, Accuracy: 0.9689, Time: 14.45 seconds\nEpoch 8/10, Loss: 0.0579, Accuracy: 0.9801, Time: 14.32 seconds\nEpoch 9/10, Loss: 0.0507, Accuracy: 0.9813, Time: 14.39 seconds\nEpoch 10/10, Loss: 0.0701, Accuracy: 0.9748, Time: 14.39 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy, precision, recall, f1 = evaluate(model_resnet18, test_loader)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T15:04:39.956548Z","iopub.execute_input":"2024-03-16T15:04:39.956909Z","iopub.status.idle":"2024-03-16T15:04:42.518723Z","shell.execute_reply.started":"2024-03-16T15:04:39.956882Z","shell.execute_reply":"2024-03-16T15:04:42.517700Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Accuracy: 0.8828\nPrecision: 0.9060\nRecall: 0.8828\nF1-score: 0.8890\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Ensemble Model","metadata":{}},{"cell_type":"code","source":"class EnsembleModel(nn.Module):\n    def __init__(self, resnet_model, mobilenet_model, densenet_model):\n        super(EnsembleModel, self).__init__()\n        self.resnet_model = resnet_model\n        self.mobilenet_model = mobilenet_model\n        self.densenet_model = densenet_model\n    \n    def forward(self, x):\n        resnet_output = self.resnet_model(x)\n        mobilenet_output = self.mobilenet_model(x)\n        densenet_output = self.densenet_model(x)\n        ensemble_output = (resnet_output + mobilenet_output + densenet_output) / 3.0  # Averaging predictions\n        return ensemble_output\n\nensemble_model = EnsembleModel(model_resnet18, model_mobilenet, model_densenet)\nensemble_model = ensemble_model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(ensemble_model.parameters(), lr=0.001)\ntrain(ensemble_model, train_loader, criterion, optimizer, num_epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T15:05:02.730712Z","iopub.execute_input":"2024-03-16T15:05:02.731525Z","iopub.status.idle":"2024-03-16T15:14:22.541247Z","shell.execute_reply.started":"2024-03-16T15:05:02.731492Z","shell.execute_reply":"2024-03-16T15:14:22.539975Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 0.0536, Accuracy: 0.9807, Time: 55.47 seconds\nEpoch 2/10, Loss: 0.0412, Accuracy: 0.9869, Time: 55.82 seconds\nEpoch 3/10, Loss: 0.0346, Accuracy: 0.9876, Time: 55.98 seconds\nEpoch 4/10, Loss: 0.0369, Accuracy: 0.9869, Time: 56.13 seconds\nEpoch 5/10, Loss: 0.0454, Accuracy: 0.9854, Time: 56.06 seconds\nEpoch 6/10, Loss: 0.0128, Accuracy: 0.9966, Time: 56.11 seconds\nEpoch 7/10, Loss: 0.0312, Accuracy: 0.9904, Time: 56.09 seconds\nEpoch 8/10, Loss: 0.0340, Accuracy: 0.9885, Time: 56.08 seconds\nEpoch 9/10, Loss: 0.0585, Accuracy: 0.9807, Time: 56.05 seconds\nEpoch 10/10, Loss: 0.0309, Accuracy: 0.9882, Time: 55.99 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy, precision, recall, f1 = evaluate(ensemble_model, test_loader)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T15:14:51.826539Z","iopub.execute_input":"2024-03-16T15:14:51.826876Z","iopub.status.idle":"2024-03-16T15:14:57.591948Z","shell.execute_reply.started":"2024-03-16T15:14:51.826851Z","shell.execute_reply":"2024-03-16T15:14:57.591034Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy: 0.9507\nPrecision: 0.9503\nRecall: 0.9507\nF1-score: 0.9498\n","output_type":"stream"}]},{"cell_type":"code","source":"till here","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Ensemble(nn.Module):\n    def __init__(self, resnet_model, mobilenet_model, densenet_model):\n        super(Ensemble, self).__init__()\n        self.resnet = resnet18(pretrained=True)\n        self.densenet = densenet121(pretrained=True)\n        self.mobilenet = mobilenet_v2(pretrained=True)\n        self.fc = nn.Linear(1000*3, 3)  # Output layer with 3 classes\n        \n    def forward(self, x):\n        features_resnet = self.resnet(x)\n        features_densenet = self.densenet(x)\n        features_mobilenet = self.mobilenet(x)\n        features_concat = torch.cat((features_resnet, features_densenet, features_mobilenet), dim=1)\n        output = self.fc(features_concat)\n        return output\n\n# Create an instance of the ensemble model\nensemble = Ensemble(model_resnet18, model_mobilenet, model_densenet)\nensemble = ensemble.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(ensemble.parameters(), lr=0.001)\ntrain(ensemble, train_loader, criterion, optimizer, num_epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T15:16:36.696255Z","iopub.execute_input":"2024-03-16T15:16:36.696620Z","iopub.status.idle":"2024-03-16T15:25:58.176133Z","shell.execute_reply.started":"2024-03-16T15:16:36.696592Z","shell.execute_reply":"2024-03-16T15:25:58.175136Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 0.8129, Accuracy: 0.7833, Time: 55.38 seconds\nEpoch 2/10, Loss: 0.2813, Accuracy: 0.8905, Time: 55.74 seconds\nEpoch 3/10, Loss: 0.1950, Accuracy: 0.9235, Time: 55.96 seconds\nEpoch 4/10, Loss: 0.1300, Accuracy: 0.9493, Time: 56.18 seconds\nEpoch 5/10, Loss: 0.1213, Accuracy: 0.9552, Time: 56.23 seconds\nEpoch 6/10, Loss: 0.1268, Accuracy: 0.9558, Time: 56.27 seconds\nEpoch 7/10, Loss: 0.0968, Accuracy: 0.9639, Time: 56.24 seconds\nEpoch 8/10, Loss: 0.0726, Accuracy: 0.9742, Time: 56.22 seconds\nEpoch 9/10, Loss: 0.0762, Accuracy: 0.9754, Time: 56.28 seconds\nEpoch 10/10, Loss: 0.0943, Accuracy: 0.9680, Time: 56.32 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy, precision, recall, f1 = evaluate(ensemble, test_loader)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T15:26:39.403462Z","iopub.execute_input":"2024-03-16T15:26:39.404283Z","iopub.status.idle":"2024-03-16T15:26:45.150119Z","shell.execute_reply.started":"2024-03-16T15:26:39.404251Z","shell.execute_reply":"2024-03-16T15:26:45.149164Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: 0.9409\nPrecision: 0.9406\nRecall: 0.9409\nF1-score: 0.9387\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(ensemble_model,'ensemble_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T15:27:20.005429Z","iopub.execute_input":"2024-03-16T15:27:20.005783Z","iopub.status.idle":"2024-03-16T15:27:20.480541Z","shell.execute_reply.started":"2024-03-16T15:27:20.005755Z","shell.execute_reply":"2024-03-16T15:27:20.479503Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Load the image and apply transformations\nimage_path = \"/kaggle/input/knee-osteoarthritis-dataset-with-severity/val/4/9070207R.png\"\nimage = Image.open(image_path)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Ensure that the image has three channels (RGB)\nif image.mode != 'RGB':\n    image = image.convert('RGB')\n\ninput_image = transform(image).unsqueeze(0)  # Add batch dimension\n\n# Normalize the image\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\ninput_image = normalize(input_image)\n\nmodel=model_resnet\n\n# Pass the image through the model\nwith torch.no_grad():\n    model.eval()\n    output = model(input_image.to(device))\n\n# Get predicted class\n_, predicted_class = torch.max(output, 1)\npredicted_class_index = predicted_class.item()\n\n# Interpret prediction\nclass_names = [\"Healthy\", \"Moderate\", \"Severe\"]  # Replace with your actual class names\npredicted_class_name = class_names[predicted_class_index]\n\nprint(f\"The model predicts the image to belong to: {predicted_class_name}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        self.densenet = densenet121(pretrained=True)\n        self.resnet = resnet50(pretrained=True)\n        self.alexnet = alexnet(pretrained=True)\n        self.fc = nn.Linear(3000, 3)  # Output layer with 3 classes\n        \n    def forward(self, x):\n        features_densenet = self.densenet(x)\n        features_resnet = self.resnet(x)\n        features_alexnet = self.alexnet(x)\n        features_concat = torch.cat((features_densenet, features_resnet, features_alexnet), dim=1)\n        output = self.fc(features_concat)\n        return output\n    \nmodel = CustomModel().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T03:40:54.633234Z","iopub.execute_input":"2024-02-27T03:40:54.634037Z","iopub.status.idle":"2024-02-27T03:40:59.005788Z","shell.execute_reply.started":"2024-02-27T03:40:54.633994Z","shell.execute_reply":"2024-02-27T03:40:59.004527Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:00<00:00, 106MB/s] \n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 112MB/s] \n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 129MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n    epoch_loss = running_loss / len(train_dataset)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-27T03:41:09.462049Z","iopub.execute_input":"2024-02-27T03:41:09.462500Z","iopub.status.idle":"2024-02-27T05:09:58.842606Z","shell.execute_reply.started":"2024-02-27T03:41:09.462463Z","shell.execute_reply":"2024-02-27T05:09:58.841659Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 1.4619\nEpoch [2/10], Loss: 0.6146\nEpoch [3/10], Loss: 0.3748\nEpoch [4/10], Loss: 0.2896\nEpoch [5/10], Loss: 0.3177\nEpoch [6/10], Loss: 0.1920\nEpoch [7/10], Loss: 0.1214\nEpoch [8/10], Loss: 0.1286\nEpoch [9/10], Loss: 0.1360\nEpoch [10/10], Loss: 0.1168\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = correct / total\nprint(f\"Accuracy on test set: {100 * accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-02-27T05:10:06.796454Z","iopub.execute_input":"2024-02-27T05:10:06.797009Z","iopub.status.idle":"2024-02-27T05:11:59.324858Z","shell.execute_reply.started":"2024-02-27T05:10:06.796956Z","shell.execute_reply":"2024-02-27T05:11:59.323850Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Accuracy on test set: 85.36%\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Load the image and apply transformations\nimage_path = \"/kaggle/input/knee-osteoarthritis-dataset-with-severity/auto_test/0/9004184_1.png\"\nimage = Image.open(image_path)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Ensure that the image has three channels (RGB)\nif image.mode != 'RGB':\n    image = image.convert('RGB')\n\ninput_image = transform(image).unsqueeze(0)  # Add batch dimension\n\n# Normalize the image\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\ninput_image = normalize(input_image)\n\n# Pass the image through the model\nwith torch.no_grad():\n    model.eval()\n    output = model(input_image.to(device))\n\n# Get predicted class\n_, predicted_class = torch.max(output, 1)\npredicted_class_index = predicted_class.item()\n\n# Interpret prediction\nclass_names = [\"Healthy\", \"Moderate\", \"Severe\"]  # Replace with your actual class names\npredicted_class_name = class_names[predicted_class_index]\nx=predicted_class_index\n\nprint(f\"The model predicts the image to belong to: {predicted_class_name,x}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-27T05:12:33.764568Z","iopub.execute_input":"2024-02-27T05:12:33.765973Z","iopub.status.idle":"2024-02-27T05:12:34.114693Z","shell.execute_reply.started":"2024-02-27T05:12:33.765926Z","shell.execute_reply":"2024-02-27T05:12:34.113711Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The model predicts the image to belong to: Healthy\n","output_type":"stream"}]},{"cell_type":"code","source":"train_path = '/kaggle/input/knee-osteoarthritis-dataset-with-severity/train'\ntest_path = '/kaggle/input/knee-osteoarthritis-dataset-with-severity/test'","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:30:08.499492Z","iopub.execute_input":"2024-03-16T10:30:08.499913Z","iopub.status.idle":"2024-03-16T10:30:08.537925Z","shell.execute_reply.started":"2024-03-16T10:30:08.499869Z","shell.execute_reply":"2024-03-16T10:30:08.536063Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nos.listdir('/kaggle/input/knee-osteoarthritis-dataset-with-severity/train')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:32:09.641781Z","iopub.execute_input":"2024-03-16T10:32:09.642235Z","iopub.status.idle":"2024-03-16T10:32:09.649986Z","shell.execute_reply.started":"2024-03-16T10:32:09.642201Z","shell.execute_reply":"2024-03-16T10:32:09.649091Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['2', '0', '3', '1', '4']"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model's state_dict\ntorch.save(model.state_dict(), \"ensemble_model.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:32:06.929275Z","iopub.execute_input":"2024-03-16T10:32:06.929635Z","iopub.status.idle":"2024-03-16T10:32:06.952068Z","shell.execute_reply.started":"2024-03-16T10:32:06.929612Z","shell.execute_reply":"2024-03-16T10:32:06.951103Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the model's state_dict\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import torch\nfrom torchvision.transforms import transforms\nfrom PIL import Image\nimport numpy as np\n\n# Load the model\nmodel = CustomModel()\nmodel.load_state_dict(torch.load(\"model.pth\"))\nmodel.eval()\n\n# Define the transformation for the image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))  # Normalize for single-channel images\n])\n\n# Load and preprocess the image\nimage_path = \"/kaggle/input/knee-osteoarthritis-dataset-with-severity/val/3/9023348L.png\"  # Change this to the path of your image\nimage = Image.open(image_path)\nimage_tensor = transform(image).unsqueeze(0)\n\n# Perform inference\nwith torch.no_grad():\n    output = model(image_tensor)\n\n# Get the predicted class\npredicted_class = torch.argmax(output).item()\n\n# Define the class labels\nclass_labels = [\"Healthy\", \"Moderate\", \"Severe\"]\n\n# Print the predicted class label\nprint(\"Predicted class:\", class_labels[predicted_class])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}